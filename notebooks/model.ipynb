{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf8d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timow\\Documents\\School\\GitHub\\JBG060_G13\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961d2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CoralSeg loaded: 4922 total samples\n",
      "       dataset  split                                         image_path  \\\n",
      "4106  CoralSeg    val  ../benthic_data\\Coralseg\\val\\Image\\PAL239_5632...   \n",
      "2532  CoralSeg  train  ../benthic_data\\Coralseg\\train\\Image\\PALWave13...   \n",
      "3480  CoralSeg  train  ../benthic_data\\Coralseg\\train\\Image\\PALWave39...   \n",
      "\n",
      "                                              mask_path  \n",
      "4106  ../benthic_data\\Coralseg\\val\\Mask\\PAL239_5632_...  \n",
      "2532  ../benthic_data\\Coralseg\\train\\Mask\\PALWave13_...  \n",
      "3480  ../benthic_data\\Coralseg\\train\\Mask\\PALWave39_...  \n"
     ]
    }
   ],
   "source": [
    "# === Section 2: Load CoralSeg Dataset ===\n",
    "\n",
    "BASE_PATH = \"../benthic_data\"\n",
    "CORALSEG_PATH = os.path.join(BASE_PATH, \"Coralseg\")\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "coralseg_data = []\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = os.path.join(CORALSEG_PATH, split, \"Image\")\n",
    "    mask_dir = os.path.join(CORALSEG_PATH, split, \"Mask\")\n",
    "\n",
    "    img_files = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n",
    "    mask_files = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
    "\n",
    "    # Match by filename\n",
    "    for img_path in img_files:\n",
    "        fname = os.path.basename(img_path).replace(\".jpg\", \"\")\n",
    "        mask_path = os.path.join(mask_dir, fname + \".png\")\n",
    "        if os.path.exists(mask_path):\n",
    "            coralseg_data.append({\n",
    "                \"dataset\": \"CoralSeg\",\n",
    "                \"split\": split,\n",
    "                \"image_path\": img_path,\n",
    "                \"mask_path\": mask_path\n",
    "            })\n",
    "\n",
    "coralseg_df = pd.DataFrame(coralseg_data)\n",
    "print(f\"‚úÖ CoralSeg loaded: {len(coralseg_df)} total samples\")\n",
    "print(coralseg_df.sample(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4dcb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Processing site: SEAFLOWER_BOLIVAR\n",
      "üìÇ Processing site: SEAFLOWER_COURTOWN\n",
      "üìÇ Processing site: SEAVIEW_ATL\n",
      "üìÇ Processing site: SEAVIEW_IDN_PHL\n",
      "üìÇ Processing site: SEAVIEW_PAC_AUS\n",
      "üìÇ Processing site: SEAVIEW_PAC_USA\n",
      "üìÇ Processing site: TETES_PROVIDENCIA\n",
      "üìÇ Processing site: UNAL_BLEACHING_TAYRONA\n",
      "‚úÖ reef_support loaded: 3311 samples across 8 sites\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "split",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mask_path",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "1a1553fa-b23a-4ba2-adef-864a8f22595e",
       "rows": [
        [
         "292",
         "SEAFLOWER_COURTOWN",
         "train",
         "../benthic_data\\reef_support\\SEAFLOWER_COURTOWN\\images\\E12_T2_C3_Corr_22sep22.JPG",
         "../benthic_data\\reef_support\\SEAFLOWER_COURTOWN\\masks_stitched\\E12_T2_C3_Corr_22sep22_mask.png"
        ],
        [
         "1943",
         "SEAVIEW_PAC_AUS",
         "train",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\images\\12025018901.jpg",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\masks_stitched\\12025018901_mask.png"
        ],
        [
         "2431",
         "SEAVIEW_PAC_USA",
         "train",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_USA\\images\\38042144001.jpg",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_USA\\masks_stitched\\38042144001_mask.png"
        ],
        [
         "3260",
         "UNAL_BLEACHING_TAYRONA",
         "train",
         "../benthic_data\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\\C9_BC_EM_T3_29nov24_CGomez_corr.jpg",
         "../benthic_data\\reef_support\\UNAL_BLEACHING_TAYRONA\\masks_stitched\\C9_BC_EM_T3_29nov24_CGomez_corr_mask.png"
        ],
        [
         "1658",
         "SEAVIEW_PAC_AUS",
         "train",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\images\\10001026502.jpg",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\masks_stitched\\10001026502_mask.png"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split</th>\n",
       "      <th>image_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>SEAFLOWER_COURTOWN</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAFLOWER_COURTOW...</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAFLOWER_COURTOW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>SEAVIEW_PAC_AUS</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\i...</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>SEAVIEW_PAC_USA</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_USA\\i...</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_USA\\m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>UNAL_BLEACHING_TAYRONA</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\UNAL_BLEACHING_TA...</td>\n",
       "      <td>../benthic_data\\reef_support\\UNAL_BLEACHING_TA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>SEAVIEW_PAC_AUS</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\i...</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dataset  split  \\\n",
       "292       SEAFLOWER_COURTOWN  train   \n",
       "1943         SEAVIEW_PAC_AUS  train   \n",
       "2431         SEAVIEW_PAC_USA  train   \n",
       "3260  UNAL_BLEACHING_TAYRONA  train   \n",
       "1658         SEAVIEW_PAC_AUS  train   \n",
       "\n",
       "                                             image_path  \\\n",
       "292   ../benthic_data\\reef_support\\SEAFLOWER_COURTOW...   \n",
       "1943  ../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\i...   \n",
       "2431  ../benthic_data\\reef_support\\SEAVIEW_PAC_USA\\i...   \n",
       "3260  ../benthic_data\\reef_support\\UNAL_BLEACHING_TA...   \n",
       "1658  ../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\i...   \n",
       "\n",
       "                                              mask_path  \n",
       "292   ../benthic_data\\reef_support\\SEAFLOWER_COURTOW...  \n",
       "1943  ../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\m...  \n",
       "2431  ../benthic_data\\reef_support\\SEAVIEW_PAC_USA\\m...  \n",
       "3260  ../benthic_data\\reef_support\\UNAL_BLEACHING_TA...  \n",
       "1658  ../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\m...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Section 3: Load reef_support datasets ===\n",
    "\n",
    "REEF_SUPPORT_PATH = os.path.join(BASE_PATH, \"reef_support\")\n",
    "\n",
    "reef_data = []\n",
    "\n",
    "# Loop through each reef site\n",
    "for site in sorted(os.listdir(REEF_SUPPORT_PATH)):\n",
    "    site_dir = os.path.join(REEF_SUPPORT_PATH, site)\n",
    "    img_dir = os.path.join(site_dir, \"images\")\n",
    "    stitched_dir = os.path.join(site_dir, \"masks_stitched\")\n",
    "    masks_dir = os.path.join(site_dir, \"masks\")\n",
    "\n",
    "    if not os.path.isdir(img_dir):\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÇ Processing site: {site}\")\n",
    "\n",
    "    # Prefer stitched masks (cleaner)\n",
    "    stitched_masks = sorted(glob.glob(os.path.join(stitched_dir, \"*.png\")))\n",
    "    for mask_path in stitched_masks:\n",
    "        fname = os.path.basename(mask_path).replace(\"_mask.png\", \"\").replace(\".png\", \"\")\n",
    "        img_candidates = glob.glob(os.path.join(img_dir, f\"{fname}.*\"))\n",
    "        if len(img_candidates) == 0:\n",
    "            continue\n",
    "        img_path = img_candidates[0]\n",
    "\n",
    "        reef_data.append({\n",
    "            \"dataset\": site,\n",
    "            \"split\": \"train\",  # no official split, will randomize later\n",
    "            \"image_path\": img_path,\n",
    "            \"mask_path\": mask_path\n",
    "        })\n",
    "\n",
    "reef_df = pd.DataFrame(reef_data)\n",
    "print(f\"‚úÖ reef_support loaded: {len(reef_df)} samples across {reef_df['dataset'].nunique()} sites\")\n",
    "reef_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d838a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_UNION_DIR = \"../coral_project_outputs/union_masks\"\n",
    "os.makedirs(SAVE_UNION_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ca1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3311/3311 [00:00<00:00, 5322.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "‚úÖ Final merged dataset size: 3276 samples\n",
      "dataset\n",
      "SEAFLOWER_BOLIVAR         245\n",
      "SEAFLOWER_COURTOWN        241\n",
      "SEAVIEW_ATL               651\n",
      "SEAVIEW_IDN_PHL           466\n",
      "SEAVIEW_PAC_AUS           657\n",
      "SEAVIEW_PAC_USA           276\n",
      "TETES_PROVIDENCIA         105\n",
      "UNAL_BLEACHING_TAYRONA    635\n",
      "dtype: int64\n",
      "üíæ Saved metadata to ../coral_project_outputs/merged_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "SAVE_UNION_DIR = \"../coral_project_outputs/union_masks\"\n",
    "os.makedirs(SAVE_UNION_DIR, exist_ok=True)\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "def make_union_mask(mask_dir, target_name):\n",
    "    \"\"\"Combine all *_mask_*.png files into one binary union mask.\"\"\"\n",
    "    masks = glob.glob(os.path.join(mask_dir, f\"{target_name}_mask_*.png\"))\n",
    "    if not masks:\n",
    "        return None\n",
    "\n",
    "    combined = None\n",
    "    for mpath in masks:\n",
    "        mask = cv2.imread(mpath, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            continue\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "        combined = mask if combined is None else np.maximum(combined, mask)\n",
    "\n",
    "    if combined is None:\n",
    "        return None\n",
    "\n",
    "    save_path = os.path.join(SAVE_UNION_DIR, f\"{target_name}_union.png\")\n",
    "    cv2.imwrite(save_path, combined * 255)\n",
    "    return save_path\n",
    "\n",
    "# Process reef_support sites (including union masks)\n",
    "for _, row in tqdm(reef_df.iterrows(), total=len(reef_df)):\n",
    "    img_path = row[\"image_path\"]\n",
    "    site_dir = os.path.dirname(os.path.dirname(img_path))\n",
    "    masks_dir = os.path.join(site_dir, \"masks\")\n",
    "    fname = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    # Try to create or find best mask\n",
    "    if os.path.exists(os.path.join(site_dir, \"masks_stitched\", f\"{fname}_mask.png\")):\n",
    "        mask_path = os.path.join(site_dir, \"masks_stitched\", f\"{fname}_mask.png\")\n",
    "    else:\n",
    "        mask_path = make_union_mask(masks_dir, fname)\n",
    "\n",
    "    if mask_path and os.path.exists(mask_path):\n",
    "        merged_data.append({\n",
    "            \"dataset\": row[\"dataset\"],\n",
    "            \"split\": \"train\",\n",
    "            \"image_path\": img_path,\n",
    "            \"mask_path\": mask_path\n",
    "        })\n",
    "print(1)\n",
    "# # Add CoralSeg dataset\n",
    "# for _, row in tqdm(coralseg_df.iterrows(), total=len(coralseg_df)):\n",
    "#     merged_data.append({\n",
    "#         \"dataset\": row[\"dataset\"],\n",
    "#         \"split\": row[\"split\"],\n",
    "#         \"image_path\": row[\"image_path\"],\n",
    "#         \"mask_path\": row[\"mask_path\"]\n",
    "#     })\n",
    "print(2)\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "\n",
    "# Clean ‚Äî remove empties\n",
    "def valid_mask(path):\n",
    "    if not os.path.exists(path):\n",
    "        return False\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    return mask is not None and mask.sum() > 0\n",
    "\n",
    "merged_df = merged_df[merged_df[\"mask_path\"].apply(valid_mask)].reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Final merged dataset size: {len(merged_df)} samples\")\n",
    "print(merged_df.groupby(\"dataset\").size())\n",
    "\n",
    "# Save CSV metadata for reuse\n",
    "CSV_PATH = \"../coral_project_outputs/merged_dataset.csv\"\n",
    "merged_df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"üíæ Saved metadata to {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa41c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reloaded merged dataset: 3276 samples\n",
      "dataset\n",
      "SEAFLOWER_BOLIVAR         245\n",
      "SEAFLOWER_COURTOWN        241\n",
      "SEAVIEW_ATL               651\n",
      "SEAVIEW_IDN_PHL           466\n",
      "SEAVIEW_PAC_AUS           657\n",
      "SEAVIEW_PAC_USA           276\n",
      "TETES_PROVIDENCIA         105\n",
      "UNAL_BLEACHING_TAYRONA    635\n",
      "dtype: int64\n",
      "‚úÖ Random sample files verified\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"../coral_project_outputs/merged_dataset.csv\"\n",
    "\n",
    "merged_df = pd.read_csv(csv_path)\n",
    "print(f\"‚úÖ Reloaded merged dataset: {len(merged_df)} samples\")\n",
    "print(merged_df.groupby(\"dataset\").size())\n",
    "\n",
    "# Optional sanity check\n",
    "sample = merged_df.sample(3, random_state=42)\n",
    "for _, row in sample.iterrows():\n",
    "    assert os.path.exists(row[\"image_path\"]), f\"Missing image {row['image_path']}\"\n",
    "    assert os.path.exists(row[\"mask_path\"]), f\"Missing mask {row['mask_path']}\"\n",
    "print(\"‚úÖ Random sample files verified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9561e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train: 2620 | Val: 656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42, stratify=None)\n",
    "print(f\"üìä Train: {len(train_df)} | Val: {len(val_df)}\")\n",
    "\n",
    "# -------------------\n",
    "# Augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# -------------------\n",
    "# Custom Dataset\n",
    "class CoralDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, \"image_path\"]\n",
    "        mask_path = self.df.loc[idx, \"mask_path\"]\n",
    "\n",
    "        # image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        # mask = np.array(Image.open(mask_path).convert(\"L\"))  # grayscale\n",
    "        # mask = (mask > 0).astype(np.float32)  # binary coral/non-coral\n",
    "        image_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 0).astype('uint8')\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"].unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# -------------------\n",
    "# Datasets & Dataloaders\n",
    "num_workers = max(os.cpu_count() - 1, 0)\n",
    "train_loader = DataLoader(\n",
    "    CoralDataset(train_df, transform=train_transform),\n",
    "    batch_size=8, shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4 if num_workers > 0 else None,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    CoralDataset(val_df, transform=val_transform),\n",
    "    batch_size=8, shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4 if num_workers > 0 else None,\n",
    ")\n",
    "\n",
    "# # -------------------\n",
    "# # Quick check\n",
    "# imgs, masks = next(iter(train_loader))\n",
    "# print(f\"‚úÖ Dataloader OK ‚Äî batch shapes: imgs {imgs.shape}, masks {masks.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1d99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timow\\Documents\\School\\GitHub\\JBG060_G13\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\timow\\.cache\\huggingface\\hub\\models--smp-hub--efficientnet-b0.imagenet. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Epoch 1/150 - Train:   0%|          | 0/328 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# === Section 6: Model setup and training (best config) ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -------------------\n",
    "# Model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None\n",
    ").to(device)\n",
    "\n",
    "# -------------------\n",
    "# Loss: BCE + Dice combo\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "dice_loss = smp.losses.DiceLoss(mode=\"binary\")\n",
    "\n",
    "def criterion(y_pred, y_true):\n",
    "    return 0.5 * bce_loss(y_pred, y_true) + 0.5 * dice_loss(y_pred, y_true)\n",
    "\n",
    "# -------------------\n",
    "# Metrics\n",
    "def iou_score(y_pred, y_true, threshold=0.5):\n",
    "    y_pred_bin = (torch.sigmoid(y_pred) > threshold).float()\n",
    "    intersection = (y_pred_bin * y_true).sum()\n",
    "    union = y_pred_bin.sum() + y_true.sum() - intersection\n",
    "    return (intersection / union).item() if union > 0 else 1.0\n",
    "\n",
    "def pixel_accuracy(y_pred, y_true, threshold=0.5):\n",
    "    y_pred_bin = (torch.sigmoid(y_pred) > threshold).float()\n",
    "    correct = (y_pred_bin == y_true).float().sum()\n",
    "    total = torch.numel(y_true)\n",
    "    return (correct / total).item()\n",
    "\n",
    "# -------------------\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# -------------------\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30, patience=5, save_path=\"../coral_project_outputs/best_merged_model.pth\"):\n",
    "    best_iou = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_iou, train_acc = 0, 0, 0\n",
    "\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Train\"):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_iou += iou_score(outputs, masks)\n",
    "            train_acc += pixel_accuracy(outputs, masks)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_iou, val_acc = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Val\"):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                val_iou += iou_score(outputs, masks)\n",
    "                val_acc += pixel_accuracy(outputs, masks)\n",
    "\n",
    "        # Averages\n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_iou /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(f\" Train Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Acc: {train_acc:.4f}\")\n",
    "        print(f\" Val   Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"  ‚úÖ Saved new best model with IoU={best_iou:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"Training complete. Best IoU: {best_iou:.4f}\")\n",
    "    return model\n",
    "\n",
    "# -------------------\n",
    "# Run training\n",
    "final_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epochs=150,\n",
    "    patience=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8074b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
