{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfaf8d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timow\\Documents\\School\\GitHub\\JBG060_G13\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961d2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CoralSeg loaded: 4922 total samples\n",
      "       dataset  split                                         image_path  \\\n",
      "2784  CoralSeg  train  ../benthic_data\\Coralseg\\train\\Image\\PALWave14...   \n",
      "2209  CoralSeg  train  ../benthic_data\\Coralseg\\train\\Image\\PALStrawn...   \n",
      "3950  CoralSeg    val  ../benthic_data\\Coralseg\\val\\Image\\FR3_8704_92...   \n",
      "\n",
      "                                              mask_path  \n",
      "2784  ../benthic_data\\Coralseg\\train\\Mask\\PALWave14_...  \n",
      "2209  ../benthic_data\\Coralseg\\train\\Mask\\PALStrawn_...  \n",
      "3950  ../benthic_data\\Coralseg\\val\\Mask\\FR3_8704_921...  \n"
     ]
    }
   ],
   "source": [
    "# === Section 2: Load CoralSeg Dataset ===\n",
    "\n",
    "BASE_PATH = \"../benthic_data\"\n",
    "CORALSEG_PATH = os.path.join(BASE_PATH, \"Coralseg\")\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "coralseg_data = []\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = os.path.join(CORALSEG_PATH, split, \"Image\")\n",
    "    mask_dir = os.path.join(CORALSEG_PATH, split, \"Mask\")\n",
    "\n",
    "    img_files = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n",
    "    mask_files = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
    "\n",
    "    # Match by filename\n",
    "    for img_path in img_files:\n",
    "        fname = os.path.basename(img_path).replace(\".jpg\", \"\")\n",
    "        mask_path = os.path.join(mask_dir, fname + \".png\")\n",
    "        if os.path.exists(mask_path):\n",
    "            coralseg_data.append({\n",
    "                \"dataset\": \"CoralSeg\",\n",
    "                \"split\": split,\n",
    "                \"image_path\": img_path,\n",
    "                \"mask_path\": mask_path\n",
    "            })\n",
    "\n",
    "coralseg_df = pd.DataFrame(coralseg_data)\n",
    "print(f\"‚úÖ CoralSeg loaded: {len(coralseg_df)} total samples\")\n",
    "print(coralseg_df.sample(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4dcb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../benthic_data\\reef_support\n",
      "üìÇ Processing site: SEAFLOWER_BOLIVAR\n",
      "üìÇ Processing site: SEAFLOWER_COURTOWN\n",
      "üìÇ Processing site: SEAVIEW_ATL\n",
      "üìÇ Processing site: SEAVIEW_IDN_PHL\n",
      "üìÇ Processing site: SEAVIEW_PAC_AUS\n",
      "üìÇ Processing site: SEAVIEW_PAC_USA\n",
      "üìÇ Processing site: TETES_PROVIDENCIA\n",
      "üìÇ Processing site: UNAL_BLEACHING_TAYRONA\n",
      "‚úÖ reef_support loaded: 3311 samples across 8 sites\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "split",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mask_path",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "60ddf156-139d-4e3e-b4a9-b739e7db0027",
       "rows": [
        [
         "2766",
         "UNAL_BLEACHING_TAYRONA",
         "train",
         "../benthic_data\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\\C1_BC_ESa_T2_29nov24_CDaza_corr.jpg",
         "../benthic_data\\reef_support\\UNAL_BLEACHING_TAYRONA\\masks_stitched\\C1_BC_ESa_T2_29nov24_CDaza_corr_mask.png"
        ],
        [
         "677",
         "SEAVIEW_ATL",
         "train",
         "../benthic_data\\reef_support\\SEAVIEW_ATL\\images\\20011059102.jpg",
         "../benthic_data\\reef_support\\SEAVIEW_ATL\\masks_stitched\\20011059102_mask.png"
        ],
        [
         "1954",
         "SEAVIEW_PAC_AUS",
         "train",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\images\\12025020301.jpg",
         "../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\masks_stitched\\12025020301_mask.png"
        ],
        [
         "978",
         "SEAVIEW_ATL",
         "train",
         "../benthic_data\\reef_support\\SEAVIEW_ATL\\images\\24032092301.jpg",
         "../benthic_data\\reef_support\\SEAVIEW_ATL\\masks_stitched\\24032092301_mask.png"
        ],
        [
         "2650",
         "TETES_PROVIDENCIA",
         "train",
         "../benthic_data\\reef_support\\TETES_PROVIDENCIA\\images\\G0088520.JPG",
         "../benthic_data\\reef_support\\TETES_PROVIDENCIA\\masks_stitched\\G0088520_mask.png"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split</th>\n",
       "      <th>image_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>UNAL_BLEACHING_TAYRONA</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\UNAL_BLEACHING_TA...</td>\n",
       "      <td>../benthic_data\\reef_support\\UNAL_BLEACHING_TA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>SEAVIEW_ATL</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_ATL\\image...</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_ATL\\masks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>SEAVIEW_PAC_AUS</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\i...</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>SEAVIEW_ATL</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_ATL\\image...</td>\n",
       "      <td>../benthic_data\\reef_support\\SEAVIEW_ATL\\masks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>TETES_PROVIDENCIA</td>\n",
       "      <td>train</td>\n",
       "      <td>../benthic_data\\reef_support\\TETES_PROVIDENCIA...</td>\n",
       "      <td>../benthic_data\\reef_support\\TETES_PROVIDENCIA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dataset  split  \\\n",
       "2766  UNAL_BLEACHING_TAYRONA  train   \n",
       "677              SEAVIEW_ATL  train   \n",
       "1954         SEAVIEW_PAC_AUS  train   \n",
       "978              SEAVIEW_ATL  train   \n",
       "2650       TETES_PROVIDENCIA  train   \n",
       "\n",
       "                                             image_path  \\\n",
       "2766  ../benthic_data\\reef_support\\UNAL_BLEACHING_TA...   \n",
       "677   ../benthic_data\\reef_support\\SEAVIEW_ATL\\image...   \n",
       "1954  ../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\i...   \n",
       "978   ../benthic_data\\reef_support\\SEAVIEW_ATL\\image...   \n",
       "2650  ../benthic_data\\reef_support\\TETES_PROVIDENCIA...   \n",
       "\n",
       "                                              mask_path  \n",
       "2766  ../benthic_data\\reef_support\\UNAL_BLEACHING_TA...  \n",
       "677   ../benthic_data\\reef_support\\SEAVIEW_ATL\\masks...  \n",
       "1954  ../benthic_data\\reef_support\\SEAVIEW_PAC_AUS\\m...  \n",
       "978   ../benthic_data\\reef_support\\SEAVIEW_ATL\\masks...  \n",
       "2650  ../benthic_data\\reef_support\\TETES_PROVIDENCIA...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Section 3: Load reef_support datasets ===\n",
    "\n",
    "REEF_SUPPORT_PATH = os.path.join(BASE_PATH, \"reef_support\")\n",
    "print(REEF_SUPPORT_PATH)\n",
    "reef_data = []\n",
    "\n",
    "# Loop through each reef site\n",
    "for site in sorted(os.listdir(REEF_SUPPORT_PATH)):\n",
    "    site_dir = os.path.join(REEF_SUPPORT_PATH, site)\n",
    "    img_dir = os.path.join(site_dir, \"images\")\n",
    "    stitched_dir = os.path.join(site_dir, \"masks_stitched\")\n",
    "    masks_dir = os.path.join(site_dir, \"masks\")\n",
    "\n",
    "    if not os.path.isdir(img_dir):\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÇ Processing site: {site}\")\n",
    "\n",
    "    # Prefer stitched masks (cleaner)\n",
    "    stitched_masks = sorted(glob.glob(os.path.join(stitched_dir, \"*.png\")))\n",
    "    for mask_path in stitched_masks:\n",
    "        fname = os.path.basename(mask_path).replace(\"_mask.png\", \"\").replace(\".png\", \"\")\n",
    "        img_candidates = glob.glob(os.path.join(img_dir, f\"{fname}.*\"))\n",
    "        if len(img_candidates) == 0:\n",
    "            continue\n",
    "        img_path = img_candidates[0]\n",
    "\n",
    "        reef_data.append({\n",
    "            \"dataset\": site,\n",
    "            \"split\": \"train\",  # no official split, will randomize later\n",
    "            \"image_path\": img_path,\n",
    "            \"mask_path\": mask_path\n",
    "        })\n",
    "\n",
    "reef_df = pd.DataFrame(reef_data)\n",
    "print(f\"‚úÖ reef_support loaded: {len(reef_df)} samples across {reef_df['dataset'].nunique()} sites\")\n",
    "reef_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d838a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_UNION_DIR = \"../coral_project_outputs/union_masks\"\n",
    "os.makedirs(SAVE_UNION_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ca1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3311/3311 [00:00<00:00, 4909.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "‚úÖ Final merged dataset size: 3276 samples\n",
      "dataset\n",
      "SEAFLOWER_BOLIVAR         245\n",
      "SEAFLOWER_COURTOWN        241\n",
      "SEAVIEW_ATL               651\n",
      "SEAVIEW_IDN_PHL           466\n",
      "SEAVIEW_PAC_AUS           657\n",
      "SEAVIEW_PAC_USA           276\n",
      "TETES_PROVIDENCIA         105\n",
      "UNAL_BLEACHING_TAYRONA    635\n",
      "dtype: int64\n",
      "üíæ Saved metadata to ../coral_project_outputs/merged_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "SAVE_UNION_DIR = \"../coral_project_outputs/union_masks\"\n",
    "os.makedirs(SAVE_UNION_DIR, exist_ok=True)\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "def make_union_mask(mask_dir, target_name):\n",
    "    \"\"\"Combine all *_mask_*.png files into one binary union mask.\"\"\"\n",
    "    masks = glob.glob(os.path.join(mask_dir, f\"{target_name}_mask_*.png\"))\n",
    "    if not masks:\n",
    "        return None\n",
    "\n",
    "    combined = None\n",
    "    for mpath in masks:\n",
    "        mask = cv2.imread(mpath, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            continue\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "        combined = mask if combined is None else np.maximum(combined, mask)\n",
    "\n",
    "    if combined is None:\n",
    "        return None\n",
    "\n",
    "    save_path = os.path.join(SAVE_UNION_DIR, f\"{target_name}_union.png\")\n",
    "    cv2.imwrite(save_path, combined * 255)\n",
    "    return save_path\n",
    "\n",
    "# Process reef_support sites (including union masks)\n",
    "for _, row in tqdm(reef_df.iterrows(), total=len(reef_df)):\n",
    "    img_path = row[\"image_path\"]\n",
    "    site_dir = os.path.dirname(os.path.dirname(img_path))\n",
    "    masks_dir = os.path.join(site_dir, \"masks\")\n",
    "    fname = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    # Try to create or find best mask\n",
    "    if os.path.exists(os.path.join(site_dir, \"masks_stitched\", f\"{fname}_mask.png\")):\n",
    "        mask_path = os.path.join(site_dir, \"masks_stitched\", f\"{fname}_mask.png\")\n",
    "    else:\n",
    "        mask_path = make_union_mask(masks_dir, fname)\n",
    "\n",
    "    if mask_path and os.path.exists(mask_path):\n",
    "        merged_data.append({\n",
    "            \"dataset\": row[\"dataset\"],\n",
    "            \"split\": \"train\",\n",
    "            \"image_path\": img_path,\n",
    "            \"mask_path\": mask_path\n",
    "        })\n",
    "print(1)\n",
    "# # Add CoralSeg dataset\n",
    "# for _, row in tqdm(coralseg_df.iterrows(), total=len(coralseg_df)):\n",
    "#     merged_data.append({\n",
    "#         \"dataset\": row[\"dataset\"],\n",
    "#         \"split\": row[\"split\"],\n",
    "#         \"image_path\": row[\"image_path\"],\n",
    "#         \"mask_path\": row[\"mask_path\"]\n",
    "#     })\n",
    "print(2)\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "\n",
    "# Clean ‚Äî remove empties\n",
    "def valid_mask(path):\n",
    "    if not os.path.exists(path):\n",
    "        return False\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    return mask is not None and mask.sum() > 0\n",
    "\n",
    "merged_df = merged_df[merged_df[\"mask_path\"].apply(valid_mask)].reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Final merged dataset size: {len(merged_df)} samples\")\n",
    "print(merged_df.groupby(\"dataset\").size())\n",
    "\n",
    "# Save CSV metadata for reuse\n",
    "CSV_PATH = \"../coral_project_outputs/merged_dataset.csv\"\n",
    "merged_df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"üíæ Saved metadata to {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa41c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reloaded merged dataset: 3276 samples\n",
      "dataset\n",
      "SEAFLOWER_BOLIVAR         245\n",
      "SEAFLOWER_COURTOWN        241\n",
      "SEAVIEW_ATL               651\n",
      "SEAVIEW_IDN_PHL           466\n",
      "SEAVIEW_PAC_AUS           657\n",
      "SEAVIEW_PAC_USA           276\n",
      "TETES_PROVIDENCIA         105\n",
      "UNAL_BLEACHING_TAYRONA    635\n",
      "dtype: int64\n",
      "‚úÖ Random sample files verified\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"../coral_project_outputs/merged_dataset.csv\"\n",
    "\n",
    "merged_df = pd.read_csv(csv_path)\n",
    "print(f\"‚úÖ Reloaded merged dataset: {len(merged_df)} samples\")\n",
    "print(merged_df.groupby(\"dataset\").size())\n",
    "\n",
    "# Optional sanity check\n",
    "sample = merged_df.sample(3, random_state=42)\n",
    "for _, row in sample.iterrows():\n",
    "    assert os.path.exists(row[\"image_path\"]), f\"Missing image {row['image_path']}\"\n",
    "    assert os.path.exists(row[\"mask_path\"]), f\"Missing mask {row['mask_path']}\"\n",
    "print(\"‚úÖ Random sample files verified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f1f7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train: 2620 | Val: 656\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42, stratify=None)\n",
    "print(f\"üìä Train: {len(train_df)} | Val: {len(val_df)}\")\n",
    "\n",
    "# -------------------\n",
    "# Augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9561e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------\n",
    "# Custom Dataset\n",
    "class CoralDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, \"image_path\"]\n",
    "        mask_path = self.df.loc[idx, \"mask_path\"]\n",
    "\n",
    "        # image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        # mask = np.array(Image.open(mask_path).convert(\"L\"))  # grayscale\n",
    "        # mask = (mask > 0).astype(np.float32)  # binary coral/non-coral\n",
    "        image_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 0).astype('uint8')\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"].unsqueeze(0).float().div(255)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3ecff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device: NVIDIA GeForce RTX 3080\n",
      "[train] iter() construction: 0.002s | first batch: 0.942s\n",
      "[val] iter() construction: 0.000s | first batch: 0.688s\n"
     ]
    }
   ],
   "source": [
    "# ===== Datasets & Dataloaders (Notebook-friendly, CUDA-aware) =====\n",
    "import os, platform, time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "\n",
    "# --- Assumes these already exist in your notebook ---\n",
    "# CoralDataset, train_df, val_df, train_transform, val_transform\n",
    "\n",
    "# 1) Basic sanity + CUDA setup\n",
    "assert torch.cuda.is_available(), \"CUDA not available but you said you're using it.\"\n",
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True  # speed up on fixed-size inputs\n",
    "\n",
    "# 2) Keep thread oversubscription in check (helps a lot with OpenCV + PyTorch workers)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 3) Notebook/Windows-aware worker policy:\n",
    "#    - In notebooks (and on Windows spawn), many workers make the *first batch* very slow.\n",
    "#    - Start conservative; you can bump to 2 later if iter time looks good.\n",
    "ON_WINDOWS = platform.system() == \"Windows\"\n",
    "IN_NOTEBOOK = True  # you're in a notebook here\n",
    "\n",
    "# Toggle: set to \"dev\" for snappy startup during experimentation,\n",
    "# switch to \"train\" when doing longer runs (can set to 2 workers).\n",
    "MODE = \"dev\"  # \"dev\" or \"train\"\n",
    "\n",
    "if MODE == \"dev\":\n",
    "    num_workers = 0  # fastest startup in notebooks\n",
    "    persistent_workers = False\n",
    "    prefetch_factor = None\n",
    "else:\n",
    "    # modest, safe default for notebooks; increase to 2 if disk & CPU keep up\n",
    "    num_workers = 1 if (IN_NOTEBOOK or ON_WINDOWS) else min(4, max(1, os.cpu_count() // 2))\n",
    "    persistent_workers = num_workers > 0\n",
    "    prefetch_factor = 2 if num_workers > 0 else None\n",
    "\n",
    "# 4) Dataloader builder\n",
    "def make_loader(df, transform, batch_size=8, shuffle=False):\n",
    "    kwargs = dict(\n",
    "        dataset=CoralDataset(df, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,               # good with CUDA; remember to use non_blocking=True on .to(device)\n",
    "        drop_last=False,\n",
    "    )\n",
    "    # only set these when workers > 0 (PyTorch will error otherwise)\n",
    "    if num_workers > 0:\n",
    "        kwargs.update(dict(\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        ))\n",
    "    return DataLoader(**kwargs)\n",
    "\n",
    "# 5) Create loaders\n",
    "BATCH_SIZE = 8\n",
    "train_loader = make_loader(train_df, train_transform, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = make_loader(val_df,   val_transform,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 6) Quick timing probe to understand where the wait happens\n",
    "def time_first_batch(loader, label):\n",
    "    t0 = time.perf_counter()\n",
    "    it = iter(loader)\n",
    "    t1 = time.perf_counter()\n",
    "    batch = next(it)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"[{label}] iter() construction: {(t1 - t0):.3f}s | first batch: {(t2 - t1):.3f}s\")\n",
    "    # Optional: move to GPU once to verify pinning benefits (use non_blocking)\n",
    "    imgs, masks = batch\n",
    "    if isinstance(imgs, torch.Tensor):\n",
    "        _ = imgs.to(device, non_blocking=True)\n",
    "    if isinstance(masks, torch.Tensor):\n",
    "        _ = masks.to(device, non_blocking=True)\n",
    "\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(device.index))\n",
    "time_first_batch(train_loader, \"train\")\n",
    "time_first_batch(val_loader,   \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd1d99e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected a `model` to be defined earlier in the notebook.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Ensure a model exists and move to GPU\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a `model` to be defined earlier in the notebook.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ---- Optimizer & (optional) scheduler ----\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected a `model` to be defined earlier in the notebook."
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Section 6 ‚Äî Model setup, losses, metrics, AMP training\n",
    "# =========================\n",
    "import os, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Device & performance knobs ----\n",
    "assert torch.cuda.is_available(), \"CUDA not available; please enable GPU.\"\n",
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")  # PyTorch >= 2.0\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Ensure a model exists and move to GPU\n",
    "assert \"model\" in globals(), \"Expected a `model` to be defined earlier in the notebook.\"\n",
    "model = model.to(device)\n",
    "\n",
    "# ---- Optimizer & (optional) scheduler ----\n",
    "LR = 3e-4  # adjust if you like\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "# (Optional) scheduler example:\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "# ---- Metrics computed on logits (apply sigmoid only for metrics) ----\n",
    "@torch.no_grad()\n",
    "def binary_metrics_from_logits(logits, targets, thr=0.5):\n",
    "    \"\"\"\n",
    "    logits: (B,1,H,W) raw outputs; targets: (B,1,H,W) float {0,1}\n",
    "    returns: (IoU, Accuracy)\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > thr)\n",
    "    targets_b = (targets > 0.5)\n",
    "\n",
    "    inter = (preds & targets_b).sum(dim=(1,2,3)).float()\n",
    "    union = (preds | targets_b).sum(dim=(1,2,3)).float()\n",
    "    iou = torch.where(union > 0, inter / union, torch.zeros_like(union)).mean().item()\n",
    "\n",
    "    acc = (preds == targets_b).float().mean().item()\n",
    "    return iou, acc\n",
    "\n",
    "# ---- Class imbalance: estimate pos_weight for BCEWithLogitsLoss ----\n",
    "@torch.no_grad()\n",
    "def estimate_pos_weight(loader, max_batches=50, device=device):\n",
    "    pos = 0.0\n",
    "    neg = 0.0\n",
    "    for i, (_, y) in enumerate(loader):\n",
    "        y = y.to(device, non_blocking=True).float()\n",
    "        pos += y.sum().item()\n",
    "        neg += y.numel() - y.sum().item()\n",
    "        if i + 1 >= max_batches:\n",
    "            break\n",
    "    pos = max(pos, 1.0); neg = max(neg, 1.0)\n",
    "    pw = neg / pos\n",
    "    print(f\"[pos_weight] positive fraction ‚âà {pos/(pos+neg):.6f}  ->  pos_weight={pw:.2f}\")\n",
    "    return torch.tensor([pw], device=device, dtype=torch.float32)\n",
    "\n",
    "pos_weight = estimate_pos_weight(train_loader)  # run once\n",
    "\n",
    "# ---- Losses: BCE logits (with pos_weight) + Dice on probabilities ----\n",
    "bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "def dice_loss_from_probs(probs, targets, eps=1e-7):\n",
    "    targets = targets.float()\n",
    "    inter = (probs * targets).sum(dim=(1,2,3))\n",
    "    union = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3))\n",
    "    dice = (2 * inter + eps) / (union + eps)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "def criterion(logits, targets):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    return 0.5 * bce_loss(logits, targets) + 0.5 * dice_loss_from_probs(probs, targets)\n",
    "\n",
    "# ---- AMP training & validation steps ----\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = total_iou = total_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for imgs, masks in pbar:\n",
    "        imgs  = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True).float()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        iou, acc = binary_metrics_from_logits(logits, masks)\n",
    "        total_loss += loss.item(); total_iou += iou; total_acc += acc; n_batches += 1\n",
    "\n",
    "    return (total_loss / n_batches, total_iou / n_batches, total_acc / n_batches)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = total_iou = total_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "    for imgs, masks in pbar:\n",
    "        imgs  = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True).float()\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "        iou, acc = binary_metrics_from_logits(logits, masks)\n",
    "        total_loss += loss.item(); total_iou += iou; total_acc += acc; n_batches += 1\n",
    "\n",
    "    return (total_loss / n_batches, total_iou / n_batches, total_acc / n_batches)\n",
    "\n",
    "# ---- Full training loop with early stopping & best-model saving ----\n",
    "def train_model(model, train_loader, val_loader, optimizer, epochs=150, patience=20, save_path=\"best_model.pth\"):\n",
    "    best_iou = -1.0\n",
    "    best_state = None\n",
    "    stale = 0\n",
    "\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}  |  LR={LR:.1e}\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_iou, tr_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        # Validate\n",
    "        va_loss, va_iou, va_acc = validate(model, val_loader, device)\n",
    "        t1 = time.time()\n",
    "\n",
    "        # (Optional) scheduler on val loss:\n",
    "        # scheduler.step(va_loss)\n",
    "\n",
    "        # Logging\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        print(f\" Train Loss: {tr_loss:.4f} | IoU: {tr_iou:.4f} | Acc: {tr_acc:.4f}\")\n",
    "        print(f\" Val   Loss: {va_loss:.4f} | IoU: {va_iou:.4f} | Acc: {va_acc:.4f}\")\n",
    "        print(f\"  ‚è±Ô∏è  epoch time: {t1 - t0:.1f}s\")\n",
    "\n",
    "        # Early stopping on IoU (use what you care about most)\n",
    "        improved = va_iou > best_iou + 1e-6\n",
    "        if improved:\n",
    "            best_iou = va_iou\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            torch.save(best_state, save_path)\n",
    "            print(f\"  ‚úÖ Saved new best model with IoU={best_iou:.4f} -> {save_path}\")\n",
    "            stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale >= patience:\n",
    "                print(f\"Early stopping (no improvement for {patience} epochs).\")\n",
    "                break\n",
    "\n",
    "    # Load best weights back into the model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# -------------------\n",
    "# Run training (edit epochs/patience as you wish)\n",
    "final_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=150,\n",
    "    patience=20,\n",
    "    save_path=\"best_model.pth\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8074b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Train:   0%|          | 0/328 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# -------------------\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m final_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\n\u001b[32m    122\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, epochs, patience, save_path)\u001b[39m\n\u001b[32m     62\u001b[39m outputs = model(imgs)\n\u001b[32m     63\u001b[39m loss = criterion(outputs, masks)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m optimizer.step()\n\u001b[32m     67\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timow\\Documents\\School\\GitHub\\JBG060_G13\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timow\\Documents\\School\\GitHub\\JBG060_G13\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timow\\Documents\\School\\GitHub\\JBG060_G13\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Section 6: Model setup and training (best config) ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -------------------\n",
    "# Model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None\n",
    ").to(device)\n",
    "\n",
    "# -------------------\n",
    "# Loss: BCE + Dice combo\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "dice_loss = smp.losses.DiceLoss(mode=\"binary\")\n",
    "\n",
    "def criterion(y_pred, y_true):\n",
    "    return 0.5 * bce_loss(y_pred, y_true) + 0.5 * dice_loss(y_pred, y_true)\n",
    "\n",
    "# -------------------\n",
    "# Metrics\n",
    "def iou_score(y_pred, y_true, threshold=0.5):\n",
    "    y_pred_bin = (torch.sigmoid(y_pred) > threshold).float()\n",
    "    intersection = (y_pred_bin * y_true).sum()\n",
    "    union = y_pred_bin.sum() + y_true.sum() - intersection\n",
    "    return (intersection / union).item() if union > 0 else 1.0\n",
    "\n",
    "def pixel_accuracy(y_pred, y_true, threshold=0.5):\n",
    "    y_pred_bin = (torch.sigmoid(y_pred) > threshold).float()\n",
    "    correct = (y_pred_bin == y_true).float().sum()\n",
    "    total = torch.numel(y_true)\n",
    "    return (correct / total).item()\n",
    "\n",
    "# -------------------\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# -------------------\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30, patience=5, save_path=\"../coral_project_outputs/best_merged_model.pth\"):\n",
    "    best_iou = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_iou, train_acc = 0, 0, 0\n",
    "\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Train\"):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_iou += iou_score(outputs, masks)\n",
    "            train_acc += pixel_accuracy(outputs, masks)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_iou, val_acc = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Val\"):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                val_iou += iou_score(outputs, masks)\n",
    "                val_acc += pixel_accuracy(outputs, masks)\n",
    "\n",
    "        # Averages\n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_iou /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(f\" Train Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Acc: {train_acc:.4f}\")\n",
    "        print(f\" Val   Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"  ‚úÖ Saved new best model with IoU={best_iou:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"Training complete. Best IoU: {best_iou:.4f}\")\n",
    "    return model\n",
    "\n",
    "# -------------------\n",
    "# Run training\n",
    "final_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epochs=25,\n",
    "    patience=4\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
